{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoders\n",
    "\n",
    "## Seoul AI Meetup, July 8\n",
    "\n",
    "Martin Kersner, <m.kersner@gmail.com>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Structure of this presentation is largely based on chapter *15: Autoencoders* from book [Hands-On Machine Learning with Scikit-Learn & Tensorflow](http://shop.oreilly.com/product/0636920052289.do)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Content\n",
    "1. Efficient Data Representation\n",
    "1. Principal Component Analysis (PCA)\n",
    "1. t-SNE\n",
    "1. Stacked Autoencoders\n",
    "1. Unsupervised Pretraining Using Stacked Autoencoders\n",
    "1. Denoising Autoencoders\n",
    "1. Sparse Autoencoders\n",
    "1. Variational Autoencoders\n",
    "1. Other Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Efficient Data Representation\n",
    "\n",
    "* Number sequences\n",
    "    * 56, 46, 8, 56, 7, 6, 8, 52,... \n",
    "    * 5, 16, 8, 4, 2, 1, 4, 2, 1,..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Efficient Data Representation\n",
    "\n",
    "* Lower Data Dimensionality\n",
    "  * Reduced computational cost\n",
    "  * Easier to train ([Curse of dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality))\n",
    "  * Easier to visualize (ND -> 3D or ND -> 2D)\n",
    "* [Factor Analysis](https://en.wikipedia.org/wiki/Factor_analysis), [Independent Component Analysis](https://en.wikipedia.org/wiki/Independent_component_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Principal Component Analysis\n",
    "\n",
    "* For unlabeled data\n",
    "* Transformation from original coordinate system to the new one\n",
    "* Orthogonal linear transformation\n",
    "* Used for dimensionality reduction\n",
    "* Principal components represent directions along which the data has the largest variations\n",
    "* [sklearn.decomposition.PCA](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Yale Face Database\n",
    "http://vision.ucsd.edu/content/yale-face-database\n",
    "\n",
    "* 15 people\n",
    "* 11 images per subject one per different facial expression or configuration\n",
    "* (center-light w/glasses, happy, left-light, w/no glasses, normal, right-light, sad, sleepy, surprised, and wink)\n",
    "\n",
    "<img src=\"files/yale-faces.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### PCA applied on Yale Face Database\n",
    "https://github.com/martinkersner/seoul-artificial-intelligence-meetup/blob/master/eigen-fisher-faces/eigen-faces.ipynb\n",
    "\n",
    "<img src=\"files/yale-pca.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.99244289  0.00755711]\n"
     ]
    }
   ],
   "source": [
    "# scikit-learn: Principal Component Analysis\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(X)\n",
    "\n",
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Autoencoders\n",
    "\n",
    "* Artificial Neural Networks\n",
    "* Same architecture as Multi-Layer Perceptron\n",
    "* Number of input neurons = Number of output neuronw\n",
    "* Trained to efficiently encode (**codings**) input information\n",
    "\n",
    "\n",
    "* Purposes\n",
    "  * Decrease dimensionality\n",
    "  * Feature detectors (unsupervised pretraining for deep neural networks)\n",
    "  * Randomly generate new data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Autoencoders\n",
    "\n",
    "* Encoder (Recognition network)\n",
    "* Decoder (Generative network)\n",
    "\n",
    "Example of **undercomplete** autoencoder.\n",
    " \n",
    " <center><img src=\"https://inspirehep.net/record/1252540/files/autoencoder.png\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Variational Autoencoders"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
